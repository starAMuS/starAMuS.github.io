#!/bin/bash
# Prepare data files for processing

set -e

echo "ğŸ” Checking for data files..."

# Check if data directories exist
if [ ! -d "data/famus" ]; then
    echo "âš ï¸  Warning: data/famus directory not found"
    echo "   Expected files: train.jsonl, dev.jsonl, test.jsonl"
fi

if [ ! -d "data/seamus" ]; then
    echo "âš ï¸  Warning: data/seamus directory not found"
    echo "   Expected files: train.json, dev.json, test.json"
fi

if [ ! -f "data/ontology.json" ]; then
    echo "âš ï¸  Warning: data/ontology.json not found"
fi

# Check for required FAMuS files
if [ -d "data/famus" ]; then
    echo "ğŸ“ Checking FAMuS data files..."
    for file in train.jsonl dev.jsonl test.jsonl; do
        if [ -f "data/famus/$file" ]; then
            echo "   âœ“ $file found"
        else
            echo "   âœ— $file missing"
        fi
    done
fi

# Check for required SEAMuS files
if [ -d "data/seamus" ]; then
    echo "ğŸ“ Checking SEAMuS data files..."
    for file in train.json dev.json test.json; do
        if [ -f "data/seamus/$file" ]; then
            echo "   âœ“ $file found"
        else
            echo "   âœ— $file missing"
        fi
    done
fi

# Check for ontology file
echo "ğŸ“ Checking ontology file..."
if [ -f "data/ontology.json" ]; then
    echo "   âœ“ ontology.json found"
else
    echo "   âœ— ontology.json missing"
fi

echo "âœ… Data check complete"
echo ""
echo "ğŸ“Œ Note: After running the processing scripts, you can use extract_urls.py to"
echo "   fetch real Wikipedia and source URLs from the MegaWika dataset."
echo "   This requires an internet connection and the 'datasets' package."